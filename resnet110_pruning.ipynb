{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.ao.nn.quantized import FloatFunctional\n",
    "\n",
    "'''\n",
    "Properly implemented ResNet-s for CIFAR10 as described in paper [1].\n",
    "\n",
    "The implementation and structure of this file is hugely influenced by [2]\n",
    "which is implemented for ImageNet and doesn't have option A for identity.\n",
    "Moreover, most of the implementations on the web is copy-paste from\n",
    "torchvision's resnet and has wrong number of params.\n",
    "\n",
    "Proper ResNet-s for CIFAR10 (for fair comparision and etc.) has following\n",
    "number of layers and parameters:\n",
    "\n",
    "name      | layers | params\n",
    "ResNet20  |    20  | 0.27M\n",
    "ResNet32  |    32  | 0.46M\n",
    "ResNet44  |    44  | 0.66M\n",
    "ResNet56  |    56  | 0.85M\n",
    "ResNet110 |   110  |  1.7M\n",
    "ResNet1202|  1202  | 19.4m\n",
    "\n",
    "which this implementation indeed has.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "[2] https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "\n",
    "If you use this implementation in you work, please don't forget to mention the\n",
    "author, Yerlan Idelbayev.\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='B'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.ff = FloatFunctional() # To make it ready for quantization\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A': # Option A does not work with torch-pruning, as the LambdaLayer cannot be analysed. Therefore we use option B\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.ff.add(out, self.shortcut(x)) # remove + for quantization\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.quant(x) # add quant\n",
    "        out = F.relu(self.bn1(self.conv1(out)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dequant(out) # add dequant\n",
    "        return out\n",
    "\n",
    "    def fuse_model(self):\n",
    "        torch.quantization.fuse_modules(self, [['conv1', 'bn1']], inplace=True)\n",
    "        for m in self.layer1:\n",
    "            torch.quantization.fuse_modules(m, [['conv1', 'bn1'], ['conv2', 'bn2']], inplace=True)\n",
    "        for m in self.layer2:\n",
    "            torch.quantization.fuse_modules(m, [['conv1', 'bn1'], ['conv2', 'bn2']], inplace=True)\n",
    "        for m in self.layer3:\n",
    "            torch.quantization.fuse_modules(m, [['conv1', 'bn1'], ['conv2', 'bn2']], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "\n",
    "def resnet32():\n",
    "    return ResNet(BasicBlock, [5, 5, 5])\n",
    "\n",
    "\n",
    "def resnet44():\n",
    "    return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "\n",
    "def resnet110():\n",
    "    return ResNet(BasicBlock, [18, 18, 18])\n",
    "\n",
    "\n",
    "def resnet1202():\n",
    "    return ResNet(BasicBlock, [200, 200, 200])\n",
    "\n",
    "\n",
    "def test(net):\n",
    "    import numpy as np\n",
    "    total_params = 0\n",
    "\n",
    "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
    "        total_params += np.prod(x.data.numpy().shape)\n",
    "    print(\"Total number of params\", total_params)\n",
    "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    for net_name in __all__:\n",
    "#        if net_name.startswith('resnet'):\n",
    "#            print(net_name)\n",
    "#            test(globals()[net_name]())\n",
    "#            print()"
   ],
   "id": "320aa762db077b55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch_pruning as tp\n",
    "\n",
    "model = resnet110()\n",
    "\n",
    "# Importance criteria\n",
    "example_inputs = torch.randn(1, 3, 32, 32)\n",
    "imp = tp.importance.TaylorImportance()\n",
    "\n",
    "ignored_layers = []\n",
    "for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Linear) and m.out_features == 10:\n",
    "        ignored_layers.append(m) # DO NOT prune the final classifier!\n",
    "\n",
    "iterative_steps = 5 # progressive pruning\n",
    "pruner = tp.pruner.MagnitudePruner(\n",
    "    model,\n",
    "    example_inputs,\n",
    "    importance=imp,\n",
    "    iterative_steps=iterative_steps,\n",
    "    ch_sparsity=0.34,\n",
    "    ignored_layers=ignored_layers,\n",
    ")\n",
    "\n",
    "base_macs, base_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "for i in range(iterative_steps):\n",
    "    if isinstance(imp, tp.importance.TaylorImportance):\n",
    "        # Taylor expansion requires gradients for importance estimation\n",
    "        loss = model(example_inputs).sum() # a dummy loss for TaylorImportance\n",
    "        loss.backward() # before pruner.step()\n",
    "    pruner.step()\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    # finetune your model here\n",
    "    # finetune(model)\n",
    "    # ...\n",
    "    # Calculate the difference in parameters between the original and pruned model\n",
    "    #print(f\"After pruning step {i + 1}:\")\n",
    "    #print(f\"Number of parameters: {nparams}\")\n",
    "    #print(f\"Difference in parameters: {base_nparams - nparams}\\n\")"
   ],
   "id": "d136f1352b9df3d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "def get_cifar10_loader(split='train', batch_size=128, workers=0, subset_size=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        split (str): 'train' or 'val'\n",
    "        batch_size (int): Batch size for DataLoader\n",
    "        workers (int): Number of worker threads\n",
    "        subset_size (int): If > 0, return a subset of the dataset\n",
    "    \"\"\"\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    if split == 'train':\n",
    "        dataset = datasets.CIFAR10(\n",
    "            root='./data', train=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomCrop(32, 4),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]), download=True)\n",
    "        shuffle = True\n",
    "    elif split == 'val':\n",
    "        dataset = datasets.CIFAR10(\n",
    "            root='./data', train=False,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]), download=True)\n",
    "        shuffle = False\n",
    "    else:\n",
    "        raise ValueError(\"split must be either 'train' or 'val'\")\n",
    "\n",
    "    if subset_size > 0:\n",
    "        dataset = torch.utils.data.Subset(dataset, range(subset_size))\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size,\n",
    "        shuffle=shuffle, num_workers=workers,\n",
    "        pin_memory=True)\n",
    "\n",
    "    return loader\n"
   ],
   "id": "fdbc8434095ef741"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate(model, val_loader, device, use_half=False):\n",
    "    import time\n",
    "    import torch.nn as nn\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    model.eval().to(device)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    start = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            if use_half:\n",
    "                inputs = inputs.half()\n",
    "                criterion = criterion.half()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(f\"Validation Accuracy: {acc:.2f}%, Avg Loss: {avg_loss:.4f}, Time: {elapsed:.2f}s\")\n",
    "    return acc, avg_loss\n"
   ],
   "id": "ed47efb7b5e67868"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, device, num_epochs=10, use_half=False):\n",
    "    model.to(device)\n",
    "    if use_half:\n",
    "        model = model.half()\n",
    "        criterion = criterion.half()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if use_half:\n",
    "                inputs = inputs.half()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            acc = 100. * correct / total\n",
    "            avg_loss = running_loss / (progress_bar.n + 1)\n",
    "            progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\", acc=f\"{acc:.2f}%\")\n",
    "'''"
   ],
   "id": "82ed6a6feaf84b37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "try:\n",
    "    from torch.amp import autocast, GradScaler\n",
    "except ImportError:\n",
    "    # fallback for older PyTorch versions\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, device, num_epochs=10, use_amp=False):\n",
    "    model.to(device)\n",
    "\n",
    "    scaler = GradScaler(enabled=(use_amp and device.type == 'cuda'))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type=device.type, enabled=use_amp and device.type == 'cuda'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            if use_amp and device.type == 'cuda':\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            acc = 100. * correct / total\n",
    "            avg_loss = running_loss / (progress_bar.n + 1)\n",
    "            progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\", acc=f\"{acc:.2f}%\")\n"
   ],
   "id": "6f3107e4da4704dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def measure_inference_time(model, dataloader, device, num_batches=100):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(5):\n",
    "            inputs, _ = next(iter(dataloader))\n",
    "            inputs = inputs.to(device)\n",
    "            _ = model(inputs)\n",
    "\n",
    "    total_time = 0.0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (inputs, _) in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "            _ = model(inputs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            total_time += (end_time - start_time)\n",
    "\n",
    "    avg_time_per_batch = total_time / num_batches\n",
    "    return avg_time_per_batch\n"
   ],
   "id": "4c2e9fac94c995a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#model = resnet110()\n",
    "model.to(\"mps\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loader = get_cifar10_loader('train', batch_size=128)\n",
    "val_loader = get_cifar10_loader('val', batch_size=128)\n",
    "val_loader_subset = get_cifar10_loader('val', batch_size=128, subset_size=1000)\n"
   ],
   "id": "112720a851c479a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_model(model, train_loader, optimizer, criterion, device, num_epochs=30)\n",
   "id": "1eac6df567407f24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "evaluate(model, val_loader, device)",
   "id": "510b0668bc0d247e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the full pruned model after all pruning steps\n",
    "torch.save(model, \"pruned_34-30_resnet110_mps.pth\")"
   ],
   "id": "3f68c20d44ceea67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the full pruned model\n",
    "pruned_model = torch.load(\"pruned_34-30_resnet110_mps.pth\", map_location=torch.device('cpu'), weights_only=False)\n",
    "\n",
    "# Set the model to evaluation mode (or training mode as needed)\n",
    "pruned_model.eval()\n",
    "print(\"Pruned model loaded successfully.\")\n"
   ],
   "id": "1d0cc5cdf14d29f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "evaluate(pruned_model, val_loader, device)",
   "id": "8213460205c89cb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in pruned_model.parameters())\n",
    "print(f\"Total number of parameters in the pruned model: {total_params}\")\n"
   ],
   "id": "c6db432a96c95af0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pruned_model.to(\"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "model_fp32 = pruned_model\n",
    "model_fp32.eval()\n",
    "\n",
    "model_fp32.fuse_model()\n",
    "\n",
    "# Sets the backend for x86\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "\n",
    "# Prepares the model for the next step i.e. calibration.\n",
    "# Inserts observers in the model that will observe the activation tensors during calibration\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32, inplace = False)\n",
    "\n",
    "evaluate(model_fp32_prepared, val_loader_subset, device)\n",
    "\n",
    "model_quantized = torch.quantization.convert(model_fp32_prepared)"
   ],
   "id": "818bb3076c3f5fd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "evaluate(model_quantized, val_loader, device)",
   "id": "f047ba89ed2ac40c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "355bfe957cca071e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pruned_model.to(\"cpu\")\n",
    "model_quantized.to(\"cpu\")\n",
    "\n",
    "\n",
    "time_float = measure_inference_time(pruned_model, val_loader, device=\"cpu\")\n",
    "time_quant = measure_inference_time(model_quantized, val_loader, device=\"cpu\")\n",
    "\n",
    "print(f\"Average inference time per batch (float model): {time_float:.4f} seconds\")\n",
    "print(f\"Average inference time per batch (quantized model): {time_quant:.4f} seconds\")\n"
   ],
   "id": "efbf7dfc741736d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4a4602222e385722"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
