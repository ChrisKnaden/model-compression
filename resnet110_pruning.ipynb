{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-05T21:09:34.106625Z",
     "start_time": "2025-04-05T21:09:33.645636Z"
    }
   },
   "source": [
    "from torch.ao.nn.quantized import FloatFunctional\n",
    "\n",
    "'''\n",
    "Properly implemented ResNet-s for CIFAR10 as described in paper [1].\n",
    "\n",
    "The implementation and structure of this file is hugely influenced by [2]\n",
    "which is implemented for ImageNet and doesn't have option A for identity.\n",
    "Moreover, most of the implementations on the web is copy-paste from\n",
    "torchvision's resnet and has wrong number of params.\n",
    "\n",
    "Proper ResNet-s for CIFAR10 (for fair comparision and etc.) has following\n",
    "number of layers and parameters:\n",
    "\n",
    "name      | layers | params\n",
    "ResNet20  |    20  | 0.27M\n",
    "ResNet32  |    32  | 0.46M\n",
    "ResNet44  |    44  | 0.66M\n",
    "ResNet56  |    56  | 0.85M\n",
    "ResNet110 |   110  |  1.7M\n",
    "ResNet1202|  1202  | 19.4m\n",
    "\n",
    "which this implementation indeed has.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "[2] https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "\n",
    "If you use this implementation in you work, please don't forget to mention the\n",
    "author, Yerlan Idelbayev.\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='B'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.ff = FloatFunctional() # To make it ready for quantization\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A': # Option A does not work with torch-pruning, as the LambdaLayer cannot be analysed. Therefore we use option B\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.ff.add(out, self.shortcut(x)) # remove + for quantization\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.quant(x) # add quant\n",
    "        out = F.relu(self.bn1(self.conv1(out)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dequant(out) # add dequant\n",
    "        return out\n",
    "\n",
    "    def fuse_model(self):\n",
    "        torch.quantization.fuse_modules(self, [['conv1', 'bn1']], inplace=True)\n",
    "        for m in self.layer1:\n",
    "            torch.quantization.fuse_modules(m, [['conv1', 'bn1'], ['conv2', 'bn2']], inplace=True)\n",
    "        for m in self.layer2:\n",
    "            torch.quantization.fuse_modules(m, [['conv1', 'bn1'], ['conv2', 'bn2']], inplace=True)\n",
    "        for m in self.layer3:\n",
    "            torch.quantization.fuse_modules(m, [['conv1', 'bn1'], ['conv2', 'bn2']], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "\n",
    "def resnet32():\n",
    "    return ResNet(BasicBlock, [5, 5, 5])\n",
    "\n",
    "\n",
    "def resnet44():\n",
    "    return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "\n",
    "def resnet110():\n",
    "    return ResNet(BasicBlock, [18, 18, 18])\n",
    "\n",
    "\n",
    "def resnet1202():\n",
    "    return ResNet(BasicBlock, [200, 200, 200])\n",
    "\n",
    "\n",
    "def test(net):\n",
    "    import numpy as np\n",
    "    total_params = 0\n",
    "\n",
    "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
    "        total_params += np.prod(x.data.numpy().shape)\n",
    "    print(\"Total number of params\", total_params)\n",
    "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    for net_name in __all__:\n",
    "#        if net_name.startswith('resnet'):\n",
    "#            print(net_name)\n",
    "#            test(globals()[net_name]())\n",
    "#            print()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:09:38.384470Z",
     "start_time": "2025-04-05T21:09:37.796721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch_pruning as tp\n",
    "\n",
    "model = resnet110()\n",
    "\n",
    "# Importance criteria\n",
    "example_inputs = torch.randn(1, 3, 32, 32)\n",
    "imp = tp.importance.TaylorImportance()\n",
    "\n",
    "ignored_layers = []\n",
    "for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Linear) and m.out_features == 10:\n",
    "        ignored_layers.append(m) # DO NOT prune the final classifier!\n",
    "\n",
    "iterative_steps = 5 # progressive pruning\n",
    "pruner = tp.pruner.MagnitudePruner(\n",
    "    model,\n",
    "    example_inputs,\n",
    "    importance=imp,\n",
    "    iterative_steps=iterative_steps,\n",
    "    ch_sparsity=0.34,\n",
    "    ignored_layers=ignored_layers,\n",
    ")\n",
    "\n",
    "base_macs, base_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "for i in range(iterative_steps):\n",
    "    if isinstance(imp, tp.importance.TaylorImportance):\n",
    "        # Taylor expansion requires gradients for importance estimation\n",
    "        loss = model(example_inputs).sum() # a dummy loss for TaylorImportance\n",
    "        loss.backward() # before pruner.step()\n",
    "    pruner.step()\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    # finetune your model here\n",
    "    # finetune(model)\n",
    "    # ...\n",
    "    # Calculate the difference in parameters between the original and pruned model\n",
    "    #print(f\"After pruning step {i + 1}:\")\n",
    "    #print(f\"Number of parameters: {nparams}\")\n",
    "    #print(f\"Difference in parameters: {base_nparams - nparams}\\n\")"
   ],
   "id": "327e9b506ea914d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophknaden/git/model-compression/.venv/lib/python3.11/site-packages/torch_pruning/pruner/algorithms/base_pruner.py:87: UserWarning: ch_sparsity is deprecated in v1.3.0. Please use pruning_ratio.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:09:41.089878Z",
     "start_time": "2025-04-05T21:09:40.624184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "def get_cifar10_loader(split='train', batch_size=128, workers=0, subset_size=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        split (str): 'train' or 'val'\n",
    "        batch_size (int): Batch size for DataLoader\n",
    "        workers (int): Number of worker threads\n",
    "        subset_size (int): If > 0, return a subset of the dataset\n",
    "    \"\"\"\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    if split == 'train':\n",
    "        dataset = datasets.CIFAR10(\n",
    "            root='./data', train=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomCrop(32, 4),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]), download=True)\n",
    "        shuffle = True\n",
    "    elif split == 'val':\n",
    "        dataset = datasets.CIFAR10(\n",
    "            root='./data', train=False,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]), download=True)\n",
    "        shuffle = False\n",
    "    else:\n",
    "        raise ValueError(\"split must be either 'train' or 'val'\")\n",
    "\n",
    "    if subset_size > 0:\n",
    "        dataset = torch.utils.data.Subset(dataset, range(subset_size))\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size,\n",
    "        shuffle=shuffle, num_workers=workers,\n",
    "        pin_memory=True)\n",
    "\n",
    "    return loader\n"
   ],
   "id": "e3169a3c67d03c39",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:09:44.950386Z",
     "start_time": "2025-04-05T21:09:44.945537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, val_loader, device, use_half=False):\n",
    "    import time\n",
    "    import torch.nn as nn\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    model.eval().to(device)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    start = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            if use_half:\n",
    "                inputs = inputs.half()\n",
    "                criterion = criterion.half()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    avg_loss = total_loss / total\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(f\"Validation Accuracy: {acc:.2f}%, Avg Loss: {avg_loss:.4f}, Time: {elapsed:.2f}s\")\n",
    "    return acc, avg_loss\n"
   ],
   "id": "6cfd2101c2b159ed",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:09:46.638200Z",
     "start_time": "2025-04-05T21:09:46.633191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, device, num_epochs=10, use_half=False):\n",
    "    model.to(device)\n",
    "    if use_half:\n",
    "        model = model.half()\n",
    "        criterion = criterion.half()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if use_half:\n",
    "                inputs = inputs.half()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            acc = 100. * correct / total\n",
    "            avg_loss = running_loss / (progress_bar.n + 1)\n",
    "            progress_bar.set_postfix(loss=f\"{avg_loss:.4f}\", acc=f\"{acc:.2f}%\")\n"
   ],
   "id": "81d8a5cb3eebbd87",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:09:48.253357Z",
     "start_time": "2025-04-05T21:09:48.250100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def measure_inference_time(model, dataloader, num_batches=100):\n",
    "    model.eval()\n",
    "    total_time = 0.0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (inputs, _) in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "            _ = model(inputs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            total_time += (end_time - start_time)\n",
    "\n",
    "    avg_time_per_batch = total_time / num_batches\n",
    "    return avg_time_per_batch\n"
   ],
   "id": "eb6161162460e842",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:09:55.596913Z",
     "start_time": "2025-04-05T21:09:53.555844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model = resnet110()\n",
    "model.to(\"mps\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loader = get_cifar10_loader('train', batch_size=128)\n",
    "val_loader = get_cifar10_loader('val', batch_size=128)\n",
    "val_loader_subset = get_cifar10_loader('val', batch_size=128, subset_size=1000)\n"
   ],
   "id": "9de43cc83a2f4b7b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:09:02.324054Z",
     "start_time": "2025-04-05T19:51:20.416482Z"
    }
   },
   "cell_type": "code",
   "source": "train_model(model, train_loader, optimizer, criterion, device, num_epochs=10, use_half=False)\n",
   "id": "8c6d01ebcaa149a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:09:23.572107Z",
     "start_time": "2025-04-05T20:09:17.981957Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate(model, val_loader, device)",
   "id": "8ba73f9061d4e817",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 76.93%, Avg Loss: 0.7282, Time: 5.58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(76.93, 0.7282403444290161)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:09:23.776283Z",
     "start_time": "2025-04-05T20:09:23.630288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the full pruned model after all pruning steps\n",
    "torch.save(model, \"resnet110.pth\")"
   ],
   "id": "ab40170d5b81e74a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:10:33.771004Z",
     "start_time": "2025-04-05T21:10:33.682801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the full pruned model\n",
    "pruned_model = torch.load(\"pruned_34-30_resnet110.pth\", map_location=torch.device('cpu'), weights_only=False)\n",
    "\n",
    "# Set the model to evaluation mode (or training mode as needed)\n",
    "pruned_model.eval()\n",
    "print(\"Pruned model loaded successfully.\")\n"
   ],
   "id": "7fafbd9978167f3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T19:31:20.686744Z",
     "start_time": "2025-04-05T19:31:20.620495Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate(pruned_model, val_loader, device)",
   "id": "2c8f8589d5b204d5",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m evaluate(model, \u001B[43mval_loader\u001B[49m, device)\n",
      "\u001B[31mNameError\u001B[39m: name 'val_loader' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:10:38.576824Z",
     "start_time": "2025-04-05T21:10:38.572045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count the total number of parameters\n",
    "total_params = sum(p.numel() for p in pruned_model.parameters())\n",
    "print(f\"Total number of parameters in the pruned model: {total_params}\")\n"
   ],
   "id": "8cbb05bdf2febd26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the pruned model: 743997\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:18:56.342026Z",
     "start_time": "2025-04-05T21:18:52.782945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pruned_model.to(\"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "model_fp32 = pruned_model\n",
    "model_fp32.eval()\n",
    "\n",
    "model_fp32.fuse_model()\n",
    "\n",
    "# Sets the backend for x86\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "\n",
    "# Prepares the model for the next step i.e. calibration.\n",
    "# Inserts observers in the model that will observe the activation tensors during calibration\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32, inplace = False)\n",
    "\n",
    "evaluate(model_fp32_prepared, val_loader_subset, device)\n",
    "\n",
    "model_quantized = torch.quantization.convert(model_fp32_prepared)"
   ],
   "id": "c6f5b0230e8e300d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 85.70%, Avg Loss: 0.4356, Time: 3.05s\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:19:20.084924Z",
     "start_time": "2025-04-05T21:19:01.059124Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate(model_quantized, val_loader, device)",
   "id": "4e307fa3b00a9d33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 85.36%, Avg Loss: 0.4500, Time: 19.02s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(85.36, 0.45002267847061156)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a249b2fe37e71cd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:21:11.065730Z",
     "start_time": "2025-04-05T21:20:27.781756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pruned_model.to(\"cpu\")\n",
    "model_quantized.to(\"cpu\")\n",
    "\n",
    "\n",
    "time_float = measure_inference_time(pruned_model, val_loader)\n",
    "time_quant = measure_inference_time(model_quantized, val_loader)\n",
    "\n",
    "print(f\"Average inference time per batch (float model): {time_float:.4f} seconds\")\n",
    "print(f\"Average inference time per batch (quantized model): {time_quant:.4f} seconds\")\n"
   ],
   "id": "9f152f1018a0b445",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per batch (float model): 0.2189 seconds\n",
      "Average inference time per batch (quantized model): 0.2050 seconds\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3f048baa5ed3ad4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
